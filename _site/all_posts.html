<!DOCTYPE html>

<html>

  <head>
	<title>All posts | data,math,code</title>
	<meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
	<!--[if lte IE 8]><script src="/home/assets/js/ie/html5shiv.js"></script><![endif]-->
	<link rel="stylesheet" href="/home/assets/css/main.css" />
	<!--[if lte IE 9]><link rel="stylesheet" href="/home/assets/css/ie9.css" /><![endif]-->
	<!--[if lte IE 8]><link rel="stylesheet" href="/home/assets/css/ie8.css" /><![endif]-->
</head>


  <body>

    <!-- Wrapper -->
<div id="wrapper">

<!-- Header -->
<header id="header">
	<a href="http://localhost:4000/home/" class="logo"><strong>data,math,code</strong> <span>carabedo</span></a>
	<nav>
		<a href="#menu">Menu</a>
	</nav>
</header>

<!-- Menu -->
<nav id="menu">
	<ul class="links">
        
		    
		
		    
		
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/home/">Home</a></li>
	    	
		
		    
		
		    
		
		    
		
		
		    
		
		    
		        <li><a href="http://localhost:4000/home/all_posts.html">All posts</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/pages/cloud.html">cloud & security</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/pages/dev.html">web development</a></li>
		    
		
		    
		
		    
		        <li><a href="http://localhost:4000/home/pages/machine.html">machine learning</a></li>
		    
		
		    
		        <li><a href="http://localhost:4000/home/pages/presentaciones.html">clases</a></li>
		    
		
	</ul>

</nav>
 
    
    
    <!-- Main -->
    <div id="main" class="alt">

      <!-- One -->
      <section id="one">
	<div class="inner">
          
	  
	  <header class="major">
	    <h1>Boosting</h1>
	  </header>
	  
	  <p>2020-09-25 00:00:00 -0300</p>
	  <p><p>En este articulo voy a dar una introduccion al aprendizaje por ensambles con principal foco en boosting. Partamos de la base de modelos sencillos como regresion lineal o logistica como estandares para la resolucion de problemas de regresion o clasificacion. Algo que nos era dificil con estos modelos era explicar comportamientos no lineales, uno debe hacer una ingeneriera de features para incorporar interacciones no lineales entre las variables. Un metodo de ajuste que funciona muy bien con las no linealidades es el arbol de decision.</p>

<h2 id="arboles-de-decision">Arboles de decision:</h2>

<p>Un arbol de decision es un metodo no parametrico de ajuste, es decir no busca una relacion funcional $ y=f(x)$ entre la variable target y las variables explicativas. Consiste en la particion iterativa del espacio de fases asignando un valor constante a cada region de manera tal de poder predecir el valor de nuevas observaciones que caigan en esta region. Su construccion es bastante intuitiva asi como su interpretacion visual.</p>

<p>Un ejemplo de clasificacion en dos variables:</p>

<p><img src="https://github.com/carabedo/carabedo.github.io/raw/main/assets/img/dt_0.png" alt="" /></p>

<p>Iterativamente vamos particionando el espacio de fases de manera de elegir las regiones que mejor ajusten a los datos.</p>

<p><img src="https://github.com/carabedo/carabedo.github.io/raw/main/assets/img/dt_1.png" alt="" /></p>

<p>Veamos como se construye con un ejemplo de regresion en una variable, tenemos estos datos con un compartamiento fuertemente no lineal:</p>

<p><img src="https://github.com/carabedo/carabedo.github.io/raw/main/assets/img/dt_2.png" alt="" /></p>

<p>Se puede ver que una regresion lineal performa muy mal, ahora ajustemos estos datos con un arbol de decision de 2 niveles:</p>

<p><img src="https://github.com/carabedo/carabedo.github.io/raw/main/assets/img/dt_5.png" alt="" /></p>

<p>Los puntos rojos representan el ajuste por el arbol de decision, parecen existir 4 rectas horizontales que ajustan los datos, hemo particionado en 4 regiones la variable x. Para cada region se le asigno un valor de la variable y. Esto permitio representar de mejor manera el comportamiento no lineal de los datos. Pero, que signifca un arbol de dos niveles? Por que solo hay cuatro regiones?</p>

<p>Aqui el grafico del arbol de decision propiamente dicho, se ve el proceso en el que fue creado. Desde una raiz se van abriendo ramas y terminan en hojas. Las hojas son las regiones finales, hay un nivel por cada vez que se particionaron regiones.</p>

<p><img src="https://github.com/carabedo/carabedo.github.io/raw/main/assets/img/dt_6.png" alt="" /></p>

<p>En las hojas, entran nuestros datos, son las 4 regiones que vemos en el grafico del ajuste. Tanto como el nivel y la cantidad de hojas son hiperparametros de los modelos de arbol de decision.</p>

<h4 id="como-elegimos-y-armamos-las-regiones">Como elegimos y armamos las regiones?</h4>

<p>Este es un proceso iterativo, se utiliza alguna metricas para decidir en cada paso una particion binaria en alguna variable. Ademas es jerarquico, en cada iteraccion se hace una eleccion de variable y de threshold que maximice el error cuadratico medio.</p>

<p>¿Cómo construye la computadora un árbol de regresión?
El enfoque ideal sería que la computadora considere todas las particiones posibles del espacio de atributos. Sin embargo esto es computacionalmente inviable, por lo que en su lugar se utiliza un algorítmo voraz (greedy) de división binaria recursiva:</p>

<p>Comenzar en la raíz del árbol.
Para cada atributo, examinar cada punto de corte posible y elegir el atributo y punto de corte de manera que el árbol resultante de hacer la división tenga el menor error cuadrático medio (ECM).
Repetir el proceso para las dos ramas resultantes y nuevamente hacer una sola división (en cada rama) para minimizar el ECM.
Repitir este proceso hasta que se cumpla un criterio de detención.
¿Cómo sabe cuándo parar?</p>

<p>Podríamos definir un criterio de detención, como la profundidad máxima del árbol o el número mínimo de muestras en la hoja.
También podríamos hacer crecer el árbol grande y luego “podarlo” utilizando algún método de poda como “cost complexity pruning”
¿Como decidir que división es la mejor?</p>

<p>Una forma de decidir cual es la mejor división es calcular la ganancia en la reduccion del error cuadrático medio, si se aplica la división candidata.</p>

\[\Delta = ECM(\text{padre}) - \sum_{j \in \text{hijos}}\frac{N_j}{N}ECM(\text{hijo}_j)\]

<p>El objetivo es buscar la maxima  $\Delta$, donde  ECM es el Error Cuadrático Medio,  $N_j$  es el número de registros en el nodo hijo j  y  N  es el número de registros en el nodo padre.</p>

<h4 id="por-que-usar-arboles-de-decision">Por que usar arboles de decision?</h4>

<p>Los árboles son muy fáciles de explicar a las personas.
Parecen más cercanos a la forma en la que las personas toman decisiones 
Pueden ser representados gráficamente, pueden ser interpretados por no expertos fácilmente (especialmente si son pequeños)
Pueden manejar fácilmente predictores cualitativos sin necesidad de crear variables dummy.</p>

<h4 id="por-que-no-usar-arboles-de-decision">Por que no usar arboles de decision?</h4>

<p>En general no tienen el mismo nivel de precisión en la predicción comparados con otros enfoques para regresión y clasificación vistos previamente. 
Además pueden ser poco robustos. Un pequeño cambio en los datos puede generar un gran cambio el árbol final estimado. Sin embargo al agregar muchos árboles de decisión usando métodos como bagging, random forests, y  boosting la performance predictiva de los árboles puede mejorarse sustancialmente.</p>

<h2 id="por-que-esambles">Por que Esambles?</h2>

<p>El problema de un modelo con un solo arbol de decision es el overfitting, una solucion a esto es usar un ensamble de arboles. Si ajustamos 50 arboles de decision y luego promediamos sus resultados podremos evitar el overfitting. A esto se lo conoce como bagging, otra manera de realizar estos ensambles es boosting.
Boosting consiste en entrenar varios modelos de manera secuencial, entrenando un nuevo modelo sobre los datos que menor permormance tuvo el modelo anterior.</p>

<h2 id="adaboost">Adaboost</h2>

<h2 id="gradient-boost">Gradient boost</h2>

<h3 id="xgboost">Xgboost</h3>

<h3 id="lightbm">Lightbm</h3>

<h3 id="catboost">Catboost</h3>

</p>
	  
          
	  
	  <header class="major">
	    <h1>Regresion Logistica</h1>
	  </header>
	  
	  <p>2020-08-29 00:00:00 -0300</p>
	  <p><p><br /></p>

<p>Miremos un problema simple de clasificacion, donde tengo datos con una sola variable explicativa y mi variable objetivo a predecir puede ser de dos clases (0,1).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">cluster_std</span><span class="o">=</span><span class="mf">0.8</span><span class="p">)</span>
</code></pre></div></div>

<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://muydipalma.github.io/home/fig0.html" height="525" width="100%"></iframe>

<p>Visualizando los datos, nos damos cuenta que no es lo eficiente ajustar esto con una recta, necesitamos algo un poco mas sofisticado. Una funcion que nos puede servir es la funcion sigmoide:</p>

<p>Como en la regresion lineal, debemos definir una funcion de costo para mis parametros y minizarla utilizando algun metodo iterativo como es el caso de descenso por gradiente. De esta manera encontraremos los parametros de la funcion sigmoide que mejor describan mis datos.
<br /></p>
<h2 id="regresion-logistica-desde-0">Regresion Logistica desde 0</h2>
<p><br /></p>

<p>Podemos implementar lo que acabo de comentar creando una clase de python, dandole el mismo formato que las clases de SKLEARN (con los metodos .fit, .predict):</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">copy</span> <span class="k">as</span> <span class="n">copy</span>

<span class="k">class</span> <span class="nc">logisticreg</span><span class="p">:</span>
    
    <span class="c1"># iteracion que busca minizar la funcion de costo haciendo descenso gradiente
</span>    
    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">__add_intercept</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        
        <span class="c1"># inicializamos los betas, ponemos betas=0             
</span>        <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        
        <span class="c1"># loop principal
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span><span class="p">):</span>
            
            <span class="c1"># calculo p inicial
</span>            <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span>
            <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">__sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
            
            <span class="c1"># calculo el gradiente
</span>            <span class="n">gradient</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">T</span><span class="p">,</span> <span class="p">(</span><span class="n">h</span> <span class="o">-</span> <span class="n">y</span><span class="p">))</span> <span class="o">/</span> <span class="n">y</span><span class="p">.</span><span class="n">size</span>
            
            <span class="c1"># asigno los nuevos betas en la direccion del gradiente
</span>            <span class="c1"># y con el paso lr
</span>            
            <span class="bp">self</span><span class="p">.</span><span class="n">theta</span> <span class="o">-=</span> <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">*</span> <span class="n">gradient</span>
            
            <span class="c1"># me guardo los betas de cada iteracion
</span>            <span class="n">p</span><span class="o">=</span><span class="n">copy</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">[:])</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">coefs</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
            <span class="n">ls</span><span class="o">=</span><span class="n">copy</span><span class="p">.</span><span class="n">copy</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">__loss</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
            <span class="bp">self</span><span class="p">.</span><span class="n">lss</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">ls</span><span class="p">)</span>
            <span class="k">if</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">verbose</span> <span class="o">==</span> <span class="bp">True</span><span class="p">):</span>
                <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">)</span>
                <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">__sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
                <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">'loss: </span><span class="si">{</span><span class="n">i</span><span class="p">,</span><span class="bp">self</span><span class="p">.</span><span class="n">__loss</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span><span class="si">}</span><span class="s"> </span><span class="se">\t</span><span class="s">'</span><span class="p">)</span>    
            <span class="c1">#listo!
</span>            
    <span class="c1">#usa los betas encontrados
</span>    <span class="k">def</span> <span class="nf">predict_prob</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">fit_intercept</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">__add_intercept</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">__sigmoid</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="bp">self</span><span class="p">.</span><span class="n">theta</span><span class="p">))</span>
    
    <span class="c1">#usamos el threshold para la clasificacion
</span>    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">predict_prob</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    
                
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span>
                 <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">num_iter</span> <span class="o">=</span> <span class="n">num_iter</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fit_intercept</span> <span class="o">=</span> <span class="n">fit_intercept</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">coefs</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">lss</span><span class="o">=</span><span class="nb">list</span><span class="p">()</span>
        
    <span class="k">def</span> <span class="nf">__add_intercept</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="n">intercept</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">intercept</span><span class="p">,</span> <span class="n">X</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
    
    <span class="c1">#sigmoide
</span>    <span class="k">def</span> <span class="nf">__sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span>
    

    <span class="c1">#funcion de costo
</span>    <span class="k">def</span> <span class="nf">__loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">(</span><span class="o">-</span><span class="n">y</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">h</span><span class="p">)).</span><span class="n">mean</span><span class="p">()</span>                
</code></pre></div></div>
<p><br />
Ahora podemos ver como van cambiando los parametros de la funcion sigmoide a medida que vamos iterando la minizacion de la funcion de costo:
<br /></p>
<iframe id="igraph" scrolling="no" style="border:none;" seamless="seamless" src="https://muydipalma.github.io/home/assets/img/figi.html" height="900" width="750"></iframe>

<p>Como se ve para la iteracion i=100000, la sigmoide se ajusta muy bien a los datos, lo que nos permite predecir a que clase pertenece una nueva observacion.</p>
</p>
	  
          
	  
	  <header class="major">
	    <h1>Algebra Lineal</h1>
	  </header>
	  <span class="image main"><img src="/home/assets/images/pic01.jpg" alt="" /></span>
	  <p>2020-08-25 00:00:00 -0300</p>
	  <p><p>Bienvenido al maravilloso mundo del machine learning! Antes de ponernos comodos veamos algunos ejemplos incomodos donde se nos manifiesta la imperiosa necesidad de usar elementos del algebra lineal. La idea de este articulo es motivar con ejemplos concretos el por que necesitamos entender y conocer sobre: vectores, matrices, tranformaciones lineales, etc…</p>

<ul>
  <li><a href="#datasets">Datasets</a></li>
  <li><a href="#imagenes-y-video">Imagenes y video</a></li>
  <li><a href="#variables-categoricas">Variables categoricas</a></li>
  <li><a href="#regresion-Lineal">Regresion Lineal</a></li>
  <li><a href="#regularizacion">Regularizacion</a></li>
  <li><a href="#pca-y-dimensionalidad">PCA y dimensionalidad</a></li>
  <li><a href="#text-mining">Text-mining</a></li>
  <li><a href="#sistemas-de-recomendacion">Sistemas de recomendacion</a></li>
  <li><a href="#inteligencia-artificial">Inteligencia artificial</a></li>
</ul>

<h3 id="datasets">Datasets</h3>

<p>La idea central de machine learning, es ajustar un modelo a un set de datos (dataset) de manera de poder predecir alguna variable (target) en funcion de otras variables explicativas (features). Para esto es necesario ordenar los datos en una tabla donde cada fila represente una observacion y que cada columna sea una feature que represente nuestra obvservacion.</p>

<p>Por ejemplo, imaginemos que queremos entrenar un modelo que nos permite predecir las ventas de un producto en funcion de la inversion que se hace en publicidad, nuestras columnas de caracteristicas seran los 3 tipos de medio de comunicacion. Television, radio y periodicos seran las features y Sales el target. Cada observacion sera un mes de los gastos. El dataset original tiene el registro de los ultimos 200 meses, aca vemos solo los ultimos 4:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TV,Radio,Newspaper,Sales
230.1,37.8,69.2,22.1
44.5,39.3,45.1,10.4
17.2,45.9,69.3,9.3
151.5,41.3,58.5,18.5
</code></pre></div></div>

<p>Esto en realidad es una <strong>matriz</strong>, una estructura basica del algebra lineal. Es mas, cuando uno parte el data set en features y target, lo que hace es tener una <em>matriz</em> (X) y un  a <strong>vector</strong> target (y). Un vector es otro elemento fundamental del algebra lineal.</p>

<p>Cada final tiene la misma longitud, esto es el mismo numero de columnas, por lo tanto decimos la que data fue vectoriazada y cada observacion nueva sera un vector o varios que serviran para predecir su variable target.</p>

<h3 id="imagenes-y-video">Imagenes y video</h3>

<p>Quizas uno ya trabaje con imagenes o este intersado en computer vision (machine learning para imagenes), para esto cada imagen puede ser pensada como una matriz (ancho x alto) donde cada lugar representa un pixel, y el valor que sera asignado tiene que ver con el color. Si la imagen fuera en blanco y negro, el valor sera en escala de grises. Pero si fuera una imagen en colores, en realidad tendriamos 3 matrices, una para cada color (RGB).</p>

<p><img src="https://github.com/muydipalma/home/raw/v3/0.png" alt="" /></p>

<p><img src="https://github.com/muydipalma/home/raw/v3/1.png" alt="" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]
[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]
[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]
[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
[1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
[1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1]
[1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1]
[1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1]
</code></pre></div></div>

<p>Asi, una imagen no es mas que otro ejemplo de el uso de una <em>matriz</em>. Entonces cualquier operacion que querramos hacer con imagenes (cropping, scaling, shearing), necesariamente nos obliga a conocer la notacion y las operaciones con matrices.</p>

<p>Y un video? Un video, por ejemplo uno que tenga 30 fotogramas por segundo. Podemos pensarlo como una sucesion de matrices, donde cada fotograma es una matriz, esta sucesion de matrices la llamaremos <strong>tensor</strong>.</p>

<h3 id="variables-categoricas">Variables categoricas</h3>

<p>A veces tenemos sistemas donde algunas variables pueden tomar valores categoricos. Imaginen un dataset con clientes de un supermercado por ejemplo:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
Genero   E.Civil   Ingresos  Sucursal   Categoria   Units   Ganancia 
   F          S      $50K       CABA   Snack Foods      5     27.38 
   M          M      $90K        CBA    Vegetables      5     14.90 
   F          M      $70K       CABA   Snack Foods      3      5.52
   M          M      $50K        MDQ         Candy      4      4.54
   F          M      $70K       CABA   Snack Foods      3      5.52
   M          M      $50K        MDQ         Candy      4      6.44
   F          M      $50K        MDQ    Vegetables      4      6.24

</code></pre></div></div>

<p>En este caso uno podria querer predecir la variable Ganancia en funcion del genero, sucursal, categoria etc!</p>

<p>Para poder aplicar cualquier tecnica de machine learning es necesario que las featues sean numericas, no ‘caba, cba o mdq’ como en el ejemplo. La solucion mas comun a esto se llama ‘one hot encoding’. Donde se representa a cada columna de variables categoricas como una tabla. En esta nueva tabla, cada valor posible de las categorias tiene asociada una columa con valores numericos.</p>

<p>Por ejemplo, para la columna sucursal (asumiendo que solo son 3 posibles sucursales) usamos la siguiente representacion:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CABA, CBA, MDQ
1, 0, 0
0, 1, 0
1, 0, 0
0, 0, 1
1, 0, 0
0, 0, 1
0, 0, 1
...
</code></pre></div></div>

<p>Cada fila ahora es un <strong>vector</strong> binario, donde solo hay un 1 que corresponde a la categoaria a la que pertenece. Esto es un ejemplo de <strong>sparse representation</strong>, todo un tema del algebra lineal. Esto se debera hacer para cada columna categorica y terminaremos con una matriz con mas columnas de las que empezamos pero conteniendo la misma informacion.</p>

<h3 id="regresion-lineal">Regresion Lineal</h3>

<p>Mas alla de la representacion de los datos, veamos un ejemplo mas concreto de machine learning. La regresion lineal es una de las herramientas mas simples para modelar la dependencia entre variables.</p>

<p>Imgenen que tenemos los siguientes datos:</p>

<p><img src="https://github.com/muydipalma/home/raw/v3/fig0.png" alt="img0" /></p>

<p>Lo primero que una observa es que hay una relacion lineal entre x e y, entonces intentaria postular lo siguiente:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>y = m . x + b

</code></pre></div></div>

<p>Existe una pendiente <em>m</em> y una ordenada al origen <em>b</em> que definen una recta que pasa por todos los puntos? Como encuentro <em>m</em> y <em>b</em> ?</p>

<p>Si esto es verdad, entonces todos los puntos</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>( x, y )
(10, 41)
(20, 32)
(30, 35)
(40, 25)
(50, 22)
(60, 18)
</code></pre></div></div>

<p>Deberian cumplir con la ecuacion de esta recta:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>41  = m . 10  +  b
32  = m . 20  +  b
35  = m . 30  +  b
25  = m . 40  +  b
22  = m . 50  +  b
18  = m . 60  +  b

</code></pre></div></div>

<p>Antes de intentar despejar <em>m</em> y <em>b</em> retoquemos un poco los datos:</p>

<p>Primero metemos un 1 multiplicando.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>41  = m . 10  +  b . 1
32  = m . 20  +  b . 1
35  = m . 30  +  b . 1
25  = m . 40  +  b . 1
22  = m . 50  +  b . 1
18  = m . 60  +  b . 1
</code></pre></div></div>
<p>Luego cambiamos de nombre:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>41  = b1 . 10  +  b0 . 1
32  = b1 . 20  +  b0 . 1
35  = b1 . 30  +  b0 . 1
25  = b1 . 40  +  b0 . 1
22  = b1 . 50  +  b0 . 1
18  = b1 . 60  +  b0 . 1
</code></pre></div></div>

<p>Ahora usando notacion matricial podemos rescribir esto como un producto de un vector por una matriz:</p>

<p><img src="https://latex.codecogs.com/png.latex?\begin{bmatrix}41\\32\\35\\25\\18\\\end{bmatrix}=\begin{bmatrix}1&amp;10\\1&amp;20\\1&amp;30\\1&amp;40\\1&amp;50\\1&amp;60\end{bmatrix}\cdot\begin{bmatrix}b_0\\b_1\end{bmatrix}" /></p>

<p>Podemos resolver este sistema de ecuaciones para despejar m y b? Si reescribimos esto en notacion matricial nos queda:</p>

<p><img src="https://latex.codecogs.com/png.latex?\large\hat{y}=X\cdot\hat{b}" /></p>

<p>Necesitamos saber de operaciones matriciales para poder “despejar” nuestro vectores de coeficientes! Lo que uno pensaria que es pasar “dividiendo” la matriz X es en realidad encontrar la inversa de esta matriz. Por otro lado, imaginemos que tenemos 10,000 puntos y 200 variables.. nuestra notacion sigue sirviendo, ahora X sera una matriz de 10000x200 y los vectores <em>x</em> e <em>y</em> 10,000x1.</p>

<h3 id="regularizacion">Regularizacion</h3>

<p>Tomemos el ultimo ejemplo, donde tenemos datos de 10,000 personas y para cada una el valor de 200 variables. Imaginemos que queremos predecir una de ellas en funcion de las otras 199.</p>

<p><img src="https://latex.codecogs.com/png.latex?\large\hat{y}=\hat{x}_0%20\cdot%20b_0%20+\hat{x}_1%20\cdot%20b_1%20+\hat{x}_2%20\cdot%20b_2%20+...+\hat{x}_{199}%20\cdot%20b_{199}" /></p>

<p>Como vimos recien podriamos hacer una regresion lineal para encontrar esos 199 coeficientes, pero que pasa si en realidad solo 4 de esas 199 fuera realmente variables explicativas y el resto solo fuera informacion relevante o redundante? Por construccion nuestro modelo con 199 variables ajustaria muy bien a nuestros datos, pero fallaria con datos nuevos por que se “aprendio de memoria” los 10,000 datos previos.</p>

<p>Existe alguna manera de encontrar cuales de esas 199 son realmente las que importan para predecir nuestra variable target? Si, una manera es usar la tecnica de regularizacion, donde se le impone condiciones a los coeficientes <em>b</em>, asi un b muy chiquito ‘casi 0’ implicara que esa variable es poco importante. Mirando la ecuacion de arriba, el termino con beta casi cero no aporta casi nada.</p>

<p>Esta nocion de ‘chico’ no sera otra cosa que controlar el tamaño del vector <em>b</em>, esto es condicionar su <strong>norma</strong>.</p>

<h3 id="pca-y-dimensionalidad">PCA y dimensionalidad</h3>

<p>Cuando el dataset tiene muchas columnas, decenas, centenas o miles puede ser un problema. No solo por que el modelo esta mas predispuesto a aprenderse de memoria los datos y no asi a capturar el comportamiento real del sistema. Si no tambien es mas costoso computacionalmente, volvemos a la misma pregunta: como elegir entonces cuales columnas son relevantes o no?</p>

<p>Los metodos para reducir de manera automatica el numero de columnas en un data set son llamados “metodos de reduccion de la dimensionalidad”, el mas popular de todos es PCA o analisis de componentes principales.</p>

<p>La idea principal de PCA es <strong>factorizar la matriz</strong> de features X, y descomponerla en nuevas variables que capturen la mayor informacion posible del sistema, siendo entre ellas <strong>linealmente independientes</strong>. De esta manera uno aproxima el sistema con las variables que mayor informacion retenga, bajando asi la dimensionalidad. En nuestro ejemplo de 199 variables podemos elegir quedarnos con la cantidad necesaria para explicar el 70% de la variabilidad, esto dependiendo del problema puede ser 5, 10, 14, etc.</p>

<h3 id="text-mining">Text-mining</h3>

<p>Una parte muy importante de machine learning es NLP (procesamiento del lenguaje natural) donde es comun trabajar con matrices de alta dimensionalidad que representen las ocurrencias de palabras en los documentos que quieren analizar.</p>

<p>Imaginemos un dataset de mails, etiquetados como SPAM y NO SPAM, nuestro objetivo es poder predecir en funcion de las palabras contenidas en el subject del mail si es o no es SPAM.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Clase    Tema

SPAM     Ofertas de esta semana.    
CLEAN    Aviso de Transferencia.
SPAM     Pida su Prestamo.
SPAM     Descuento en pañales.
CLEAN    FWD: Fotos Cumpleaños
CLEAN    RE: Consulta Algebra lineal
SPAM     COTO TE CONOCE

</code></pre></div></div>

<p>Para estro primero tenemos que codificar nuestras features como lo hicimos para las variables categoricas, con esa misma idea ahora tendriamos una columna por cada palabra presente en TODO el dataset. Entonces cada mail sera representado por un vector larguisimo con ceros y unos dependiendo de las palabras presentes en el.</p>

<p>Veamos como ejemplo, la representacion de los primeros 2 mails:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Clase    ['Ofertas', 'de', 'esta', 'semana', 'Aviso',  'Transferencia', 'Pida', 'su', 'pañales',..., 'CONOCE']

SPAM     [1,          1,      1,        1,        0,                0,  ,   0,     0,          0, ...      0]
CLEAN    [0,          1,      0,        0,        1,                1,  ,   0,     0,          0, ...      0]

</code></pre></div></div>

<p>Como vemos, tamaño de esta matriz de features dependera de la cantidad de palabras presentes en los mails, mientras mas mails, mas palabras, mayor dimensionalidad. El diccionario de la RAE contiene 88.000 palabras…imaginen el tamaño de esa matriz si hablamos por ejemplo de una base de datos de 100,000 mails.</p>

<p>Sin embargo estas matrices tendran muchisimos 0, por lo que la informacion estara muy dispersa, a esto se lo llama <strong>sparse matrix representation</strong>. Para poder manejar este tipo de estructuras y reducirla es necesario usar  <strong>factorizacion matricial</strong>, como SVD o <strong>singular-value decomposition</strong> para poder reducir el tamaño concentrando toda la informacion. Con esta representacion vemos nuevamente como podemos usar todas las propiedades de operaciones vectoriales y matriciales para analizar texto maximizando la eficiencia del procesamiento, a esto se lo conoce como  <strong>Latent Semantic Analysis</strong>.</p>

<h3 id="sistemas-de-recomendacion">Sistemas de recomendacion</h3>

<p>Como hace tiktok para enviciarte con videitos hipnoticos uno detras de otro? como que que es tiktok? bueno bueno, youtube, netflix, spotify, el que quieras, como le hacen? En machine learning predecir que sugerencia va a tener mayor impacto se lo suele llamar “sistemas de recomendacion”.</p>

<p>El ejemplo mas burdo es mercado libre, bombardeandote con ofertas similares a tu ultima busqueda, buscate una vez el precio de una lampara… listo para mercadolibre ahora queres poner un local de EL EMPORIO DE LAS LAMPARAS. Esta similaridad, en principio basica de simplemente repetir la ultima busqueda se puede mejorar si definimos mejor que es algo ‘similar’.</p>

<p>Abstrayendonos como hace un rato donde cada mail era un vector, uno puede pensar en que cada producto de mercado libre es un vector y entonces podriamos ofrecer vectores que maximizen esta similitud con ultimo vector buscado. Para este tipo de problemas es mas util medir la similitud de vectores por <strong>similitud coseno</strong>.</p>

<p>Imagenemos los siguientes datos de nuestro almacen:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cliente    productos

user_1         1x pan,      1x salchica,    1x jugo
user_2       100x pan,  100x salchichas,  100x jugo
user_3         1x pan,       1x cerveza,    1x paty
</code></pre></div></div>

<p>La similitud coseno tiene en cuenta el angulo entre los vectores, la similitud euclidia (la norma de la que hablamos antes) nos mediria la distancia del vector diferencia. En este sentido, usando la similitud coseno, el cliente 1 es mas similar al cliente 2. Por otra parte, usando la similitud euclidia, el cliente 1 es mas similiar al cliente 3.</p>

<h3 id="inteligencia-artificial">Inteligencia Artificial</h3>

<p>Inteligencia Artificial! Eso de lo que todo mundo habla y teme, que no es mas que un arreglo <em>profundo</em> de redes neuronales. Debido a la evolucion del hardware es posible entrenar grandes volumenes de datos y muchas capas (por eso el deep). Los metodos de redes neuronales profundas o <em>Deep learning</em> son el ultimo grito de la moda! (en la jerga se estila usar <em>state-of-the-art</em> ) obteniendo permonfaces nunca antes alcanzadas en todo tipo de problemas, analisis de imagenes, videos, texto, audio.</p>

<p>En el fondo estas redes neuronales son representadas como matrices, vectores y tensores, donde un tensor no es otra cosa que una matriz en 3D (como lo vimos en el ejemplo de videos). Es por esto que algebra lineal es central para la descripcion de los metodos de deep learning, toda la documentacion de las implementaciones estan dadas en notacion tensorial, incluso una de las libreras mas populares se llama <em>TensorFlow</em>.</p>

<h2 id="resumiendo">Resumiendo:</h2>

<p>Imagino que despues de todos estos ejemplos, estan convencidos de lo necesario que es saber algebra lineal. El lenguaje basico detras de todas las ideas de machine learning son los vectores matricas y transformaciones lineales. Sin esto, es muy dificil y menos provechosa cualquier texto o clase de ML.</p>

<p>Aca una playlist de youtube con algunos conceptos basicos del algebra lineal:</p>

<p><a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">aca</a></p>

</p>
	  
          
	  
	  <header class="major">
	    <h1>Que es Data Science?</h1>
	  </header>
	  
	  <p>2020-08-20 00:00:00 -0300</p>
	  <p><p>Tal vez es mas facil decir que NO es data science.</p>

<h3 id="no-es-algo-nuevo">No es algo nuevo:</h3>

<p>Si bien hoy todo el mundo habla de data science, machine learning, inteligencia artificial como la nueva solucion a todo. Data science es un rejunte de años y años de investigacion en el desarrollo de metodos y herramientas estadisticas.</p>

<ul>
  <li><a href="https://en.wikipedia.org/wiki/Least_squares">Cuadrados minimos</a> (1805)</li>
  <li><a href="https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm">Knn</a> (1967)</li>
  <li>Random Forest (1995)</li>
  <li><a href="https://xgboost.readthedocs.io/en/latest/">Xgboost</a> (2014)</li>
  <li>Lightgbm (2017)</li>
  <li><a href="https://catboost.ai/">Catboost</a> (2017)</li>
  <li>Yolo 3 (2018)</li>
  <li>GPT-3 (2020)</li>
</ul>

<p>Pero entonces, por que aparece recien? La respuesta es el volumen de los datos en las ultimas decadas la aculumacion y produccion exponencial de datos, permite utilizar todas esas herramientas estadisticas en problemas concretos. Data Science aparace por la necesidad de administrar y entender todo este volumen de informacion. Con el aumento de los dispositivos interconectados atraves de la internet la cantidad de informacion sigue creciendo de manera exponencial.</p>

<h3 id="no-es-business-intelligence">No es Business Intelligence:</h3>

<p>BI y la ciencia de datos a menudo se confunden, sin embargo, no son sinónimos. La BI usa datos, sí, pero se trata más de los aspectos operativos y particulares de su organización. A través de este proceso, responde preguntas como qué, cuándo, cómo o quién. Por ejemplo: aprender más sobre sus clientes y su audiencia. Por otro lado, la ciencia de datos tiene más que ver con el <strong>análisis predictivo</strong> que busca generalidades. El objetivo es el descubrimiento de conocimiento, de patrones latentes en los datos que permitan describir aspectos de un sistema que antes se desconocian. Por ejemplo, la ciencia de datos puede ayudar a comprender por qué sucedió algo o cuándo volverá a ocurrir. Además, la ciencia de datos puede responder qué sucederá si se cambian algunas variables de un proceso o plan de negocios. Por esta razón, la ciencia de datos se trata más de minería de datos y análisis estadístico o cuantitativo.</p>

<h3 id="no-es-magia">No es magia:</h3>

<p>El marketing que tiene data science le juega en contra, todos creen que data science es una herramienta magica que soluciona todo tipo de problemas. Me hace acordar a mi madre que hace 15 años que esta convencida que todo se soluciona googleando (bueno, hoy en dia eso es real…una visionaria!). Ni siquiera es una ciencia en si misma, es una interdisciplina que toma herramientas de la Estadísticas, del Análisis Multivariable, del Algebra Lineal  y la Computacion. No solo eso, por otra parte es todo un arte cuando se la implementa en problemas reales.</p>

<p>Para extraer el tipo de información utilizable que se necesita en una empresas, ademas de la teoria matematica se necesita una amplia experiencia en el negocio para la interpretacion final de los modelos y ni hablar de la ingenieria de features para saber que datos serviran y cuales no. Si bien es verdad que la implementación de herramientas predictivas y de aprendizaje automático puede compensar esto un poco. Aún asi se necesita científicos de datos con experiencia en el negocio que estén bien informados y tengan experiencia con el tipo de información específica de su negocio.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Esto es importantisimo, por mas conocimiento de la matematica y la programacion que uno tenga, 
no puede encarar solo/sola un proyecto con datos de un area que no tenga experiencia.
</code></pre></div></div>

<h3 id="no-es-necesario-ser-un-programador">No es necesario ser un programador:</h3>

<p>Sí, hay muchos científicos de datos que saben cómo codear y trabajar con diferentes lenguajes de programacion. Pero, cuan necesario es realmente eso?. Actualmente hay librerias en diferentes lenguajes muy bien implementadas y documentadas. Por ejemplo, en python <strong>sklearn</strong> la popularidad de este lenguaje permite que exista muchisima documentacion al respecto y una gran comunidad detras. Haciendo que sea muy sencillo implementar modelos y lidiar con los problemas que puedan aparecer en el camino. Nada que googleando 10 minutos no se pueda resolver, sin necesidad de ser un experto programador en C++.</p>

<h3 id="no-es-necesario-ser-un-cientifico">No es necesario ser un cientifico?:</h3>

<p>Para algunas tareas avanzadas en data science como machine learning y el deep learning, un conocimiento avanzado de algebra lineal, calculo multivariable, estadistica y probabilidad ayudan. Esto no implica que las personas que no tienen un título en matemáticas o estadística no puedan convertirse en científicos de datos.</p>

<p>Hoy en día, el mercado se enfrentan a una grave escasez de profesionales de datos capaces de aprovechar los datos para obtener información empresarial útil. Esto ha llevado al surgimiento de científicos de datos ‘blandos’, es decir, profesionales que no hayan tenido una formacion ‘dura’ en matematica o ciencia ( matematicos, computologos, fisicos, etc) , pero que pueden usar las herramientas y técnicas de ciencia de datos para crear modelos de datos eficientes. Estos científicos de datos si bien no son expertos en estadísticas y matemáticas, conocen las herramientas de adentro hacia afuera, hacen las preguntas correctas y tienen una intuicion acertada de los conceptos matematicos detras de las herramientas.</p>

<p>Ojo! Si uno es un ‘blandito’ tiene que hacer el esfuerzo e invertir tiempo en ponerse al dia con la matematica. Es realmente necesaria una vision integral de las herramientas de machine learning para poder tener una solida formacion. Esto es solo posible si al menos una vez uno se sienta con un lapiz y papel a convencerse de conceptos basicos como Gradientes, Autovalores, Autovectores, Transformaciones Lineales, etc…</p>

<p>Es decir, es fuertemente recomendable algun <a href="https://www.coursera.org/specializations/mathematics-machine-learning">cursito</a> gratis de algebra lineal y uno de calculo multivariable antes de meterse al mundo de los datos. Esta es una de las mejores inversiones que se puede hacer, sera mucho mas disfrutable y aprovechable cualquier curso de data science si uno/una lo encara con una base matematica solida.</p>

<p><a href="https://carabedo.github.io/lineal.html">Aca</a> un articulo donde cuento donde aparece el algebra lineal en machine learning.</p>
</p>
	  
          
	  
	  <header class="major">
	    <h1>Tempus</h1>
	  </header>
	  <span class="image main"><img src="/home/assets/images/pic02.jpg" alt="" /></span>
	  <p>2016-08-24 00:00:00 -0300</p>
	  <p><p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. 
Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. 
Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. 
Curabitur sapien risus, commodo eget turpis at, elementum convallis elit. Pellentesque enim turpis, hendrerit.</p>

<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. 
Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. 
Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. 
Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. 
Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. 
Pellentesque aliquam maximus risus, vel sed vehicula.</p>

<p>Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. 
Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fersapien risus, commodo eget turpis at, elementum convallis elit. 
Pellentesque enim turpis, hendrerit tristique lorem ipsum dolor.</p>
</p>
	  
          
	  
	  <header class="major">
	    <h1>Magna</h1>
	  </header>
	  <span class="image main"><img src="/home/assets/images/pic03.jpg" alt="" /></span>
	  <p>2016-08-23 00:00:00 -0300</p>
	  <p><p><img src="http://localhost:4000/assets/images/pic03.jpg" alt="test image" /></p>

<p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis.
Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat.
Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris.
Curabitur sapien risus, commodo eget turpis at, elementum convallis elit. Pellentesque enim turpis, hendrerit.</p>

<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis.
Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum.
Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum.
Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus.
Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus.
Pellentesque aliquam maximus risus, vel sed vehicula.</p>

<p>Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis.
Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fersapien risus, commodo eget turpis at, elementum convallis elit.
Pellentesque enim turpis, hendrerit tristique lorem ipsum dolor.</p>
</p>
	  
          
	  
	  <header class="major">
	    <h1>Ipsum</h1>
	  </header>
	  <span class="image main"><img src="/home/assets/images/pic04.jpg" alt="" /></span>
	  <p>2016-08-22 00:00:00 -0300</p>
	  <p><p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. 
Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. 
Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. 
Curabitur sapien risus, commodo eget turpis at, elementum convallis elit. Pellentesque enim turpis, hendrerit.</p>

<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. 
Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. 
Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. 
Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. 
Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. 
Pellentesque aliquam maximus risus, vel sed vehicula.</p>

<p>Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. 
Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fersapien risus, commodo eget turpis at, elementum convallis elit. 
Pellentesque enim turpis, hendrerit tristique lorem ipsum dolor.</p>
</p>
	  
          
	  
	  <header class="major">
	    <h1>Consequat</h1>
	  </header>
	  <span class="image main"><img src="/home/assets/images/pic05.jpg" alt="" /></span>
	  <p>2016-08-21 00:00:00 -0300</p>
	  <p><p>Donec eget ex magna. Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. 
Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fergiat. 
Pellentesque in mi eu massa lacinia malesuada et a elit. Donec urna ex, lacinia in purus ac, pretium pulvinar mauris. 
Curabitur sapien risus, commodo eget turpis at, elementum convallis elit. Pellentesque enim turpis, hendrerit.</p>

<p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis dapibus rutrum facilisis. 
Class aptent taciti sociosqu ad litora torquent per conubia nostra, per inceptos himenaeos. Etiam tristique libero eu nibh porttitor fermentum. 
Nullam venenatis erat id vehicula viverra. Nunc ultrices eros ut ultricies condimentum. 
Mauris risus lacus, blandit sit amet venenatis non, bibendum vitae dolor. Nunc lorem mauris, fringilla in aliquam at, euismod in lectus. 
Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. In non lorem sit amet elit placerat maximus. 
Pellentesque aliquam maximus risus, vel sed vehicula.</p>

<p>Interdum et malesuada fames ac ante ipsum primis in faucibus. Pellentesque venenatis dolor imperdiet dolor mattis sagittis. 
Praesent rutrum sem diam, vitae egestas enim auctor sit amet. Pellentesque leo mauris, consectetur id ipsum sit amet, fersapien risus, commodo eget turpis at, elementum convallis elit. 
Pellentesque enim turpis, hendrerit tristique lorem ipsum dolor.</p>
</p>
	  
          
	</div>
      </section>

    </div>

    <!-- Contact -->



<!-- Footer -->
	<footer id="footer">
		<div class="inner">
			<ul class="icons">
				
					
						<li>
							<a href="https://github.com/carabedo" class="icon alt fa-github" target="_blank" rel="noopener noreferrer" aria-label="GitHub">
								<span class="label">GitHub</span>
							</a>
						</li>
					
				
			</ul>
			<ul class="copyright">
				<li>Design: <a href="https://html5up.net" target="_blank">HTML5 UP</a></li>
		

			</ul>
		</div>
	</footer>

</div>

<!-- Scripts -->
	<script src="http://localhost:4000/home/assets/js/jquery.min.js"></script>
	<script src="http://localhost:4000/home/assets/js/jquery.scrolly.min.js"></script>
	<script src="http://localhost:4000/home/assets/js/jquery.scrollex.min.js"></script>
	<script src="http://localhost:4000/home/assets/js/skel.min.js"></script>
	<script src="http://localhost:4000/home/assets/js/util.js"></script>
	<!--[if lte IE 8]><script src="http://localhost:4000/home/assets/js/ie/respond.min.js"></script><![endif]-->
	<script src="http://localhost:4000/home/assets/js/main.js"></script>


  </body>

</html>
